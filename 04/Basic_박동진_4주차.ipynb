{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "\n",
        "data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "data_df.head(3)\n",
        "# 개별 모델은 로지스틱 회귀와 KNN 임. \n",
        "lr_clf = LogisticRegression(solver='liblinear')\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=8)\n",
        "\n",
        "# 개별 모델을 소프트 보팅 기반의 앙상블 모델로 구현한 분류기 \n",
        "vo_clf = VotingClassifier( estimators=[('LR',lr_clf),('KNN',knn_clf)] , voting='soft' )\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, \n",
        "                                                    test_size=0.2 , random_state= 156)\n",
        "\n",
        "# VotingClassifier 학습/예측/평가. \n",
        "vo_clf.fit(X_train , y_train)\n",
        "pred = vo_clf.predict(X_test)\n",
        "print('Voting 분류기 정확도: {0:.4f}'.format(accuracy_score(y_test , pred)))\n",
        "\n",
        "# 개별 모델의 학습/예측/평가.\n",
        "classifiers = [lr_clf, knn_clf]\n",
        "for classifier in classifiers:\n",
        "    classifier.fit(X_train , y_train)\n",
        "    pred = classifier.predict(X_test)\n",
        "    class_name= classifier.__class__.__name__\n",
        "    print('{0} 정확도: {1:.4f}'.format(class_name, accuracy_score(y_test , pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oZwyh70h41m",
        "outputId": "e5c404d8-ba8a-40ab-c936-0046a6d2c7c2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting 분류기 정확도: 0.9561\n",
            "LogisticRegression 정확도: 0.9474\n",
            "KNeighborsClassifier 정확도: 0.9386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#rf\n",
        "def get_new_feature_name_df(old_feature_name_df):\n",
        "    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(),\n",
        "                                  columns=['dup_cnt'])\n",
        "    feature_dup_df = feature_dup_df.reset_index()\n",
        "    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')\n",
        "    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1]) \n",
        "                                                                                         if x[1] >0 else x[0] ,  axis=1)\n",
        "    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)\n",
        "    return new_feature_name_df\n",
        "import pandas as pd\n",
        "\n",
        "def get_human_dataset( ):\n",
        "    \n",
        "    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.\n",
        "    feature_name_df = pd.read_csv('./human_activity/features.txt',sep='\\s+',\n",
        "                        header=None,names=['column_index','column_name'])\n",
        "    \n",
        "    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성. \n",
        "    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n",
        "    \n",
        "    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환\n",
        "    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n",
        "    \n",
        "    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용\n",
        "    X_train = pd.read_csv('./human_activity/train/X_train.txt',sep='\\s+', names=feature_name )\n",
        "    X_test = pd.read_csv('./human_activity/test/X_test.txt',sep='\\s+', names=feature_name)\n",
        "    \n",
        "    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여\n",
        "    y_train = pd.read_csv('./human_activity/train/y_train.txt',sep='\\s+',header=None,names=['action'])\n",
        "    y_test = pd.read_csv('./human_activity/test/y_test.txt',sep='\\s+',header=None,names=['action'])\n",
        "    \n",
        "    # 로드된 학습/테스트용 DataFrame을 모두 반환 \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = get_human_dataset()\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 결정 트리에서 사용한 get_human_dataset( )을 이용해 학습/테스트용 DataFrame 반환\n",
        "X_train, X_test, y_train, y_test = get_human_dataset()\n",
        "\n",
        "# 랜덤 포레스트 학습 및 별도의 테스트 셋으로 예측 성능 평가\n",
        "rf_clf = RandomForestClassifier(random_state=0)\n",
        "rf_clf.fit(X_train , y_train)\n",
        "pred = rf_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test , pred)\n",
        "print('랜덤 포레스트 정확도: {0:.4f}'.format(accuracy))\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {\n",
        "    'n_estimators':[100],\n",
        "    'max_depth' : [6, 8, 10, 12], \n",
        "    'min_samples_leaf' : [8, 12, 18 ],\n",
        "    'min_samples_split' : [8, 16, 20]\n",
        "}\n",
        "# RandomForestClassifier 객체 생성 후 GridSearchCV 수행\n",
        "rf_clf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
        "grid_cv = GridSearchCV(rf_clf , param_grid=params , cv=2, n_jobs=-1 )\n",
        "grid_cv.fit(X_train , y_train)\n",
        "\n",
        "print('최적 하이퍼 파라미터:\\n', grid_cv.best_params_)\n",
        "print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))\n",
        "rf_clf1 = RandomForestClassifier(n_estimators=300, max_depth=10, min_samples_leaf=8, \\\n",
        "                                 min_samples_split=8, random_state=0)\n",
        "rf_clf1.fit(X_train , y_train)\n",
        "pred = rf_clf1.predict(X_test)\n",
        "print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test , pred)))\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "ftr_importances_values = rf_clf1.feature_importances_\n",
        "ftr_importances = pd.Series(ftr_importances_values,index=X_train.columns  )\n",
        "ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.title('Feature importances Top 20')\n",
        "sns.barplot(x=ftr_top20 , y = ftr_top20.index)\n",
        "fig1 = plt.gcf()\n",
        "plt.show()\n",
        "plt.draw()\n",
        "fig1.savefig('rf_feature_importances_top20.tif', format='tif', dpi=300, bbox_inches='tight')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "NtyBsHebh7hE",
        "outputId": "cc4fdf74-d2f7-4e5a-ce7d-12d7c600ee61"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-7e6c42ab25fd>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_human_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-7e6c42ab25fd>\u001b[0m in \u001b[0;36mget_human_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     feature_name_df = pd.read_csv('./human_activity/features.txt',sep='\\s+',\n\u001b[0m\u001b[1;32m     17\u001b[0m                         header=None,names=['column_index','column_name'])\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './human_activity/features.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "X_train, X_test, y_train, y_test = get_human_dataset()\n",
        "\n",
        "# GBM 수행 시간 측정을 위함. 시작 시간 설정.\n",
        "start_time = time.time()\n",
        "\n",
        "gb_clf = GradientBoostingClassifier(random_state=0)\n",
        "gb_clf.fit(X_train , y_train)\n",
        "gb_pred = gb_clf.predict(X_test)\n",
        "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
        "\n",
        "print('GBM 정확도: {0:.4f}'.format(gb_accuracy))\n",
        "print(\"GBM 수행 시간: {0:.1f} 초 \".format(time.time() - start_time))\n",
        "### 아래는 책에서 설명드리지는 않지만 GridSearchCV로 GBM의 하이퍼 파라미터 튜닝을 수행하는 예제 입니다. \n",
        "### 사이킷런이 1.X로 업그레이드 되며서 GBM의 학습 속도가 현저하게 저하되는 문제가 오히려 발생합니다. \n",
        "### 아래는 수행 시간이 오래 걸리므로 참고용으로만 사용하시면 좋을 것 같습니다. \n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {\n",
        "    'n_estimators':[100, 500],\n",
        "    'learning_rate' : [ 0.05, 0.1]\n",
        "}\n",
        "grid_cv = GridSearchCV(gb_clf , param_grid=params , cv=2 ,verbose=1)\n",
        "grid_cv.fit(X_train , y_train)\n",
        "print('최적 하이퍼 파라미터:\\n', grid_cv.best_params_)\n",
        "print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))\n",
        "# GridSearchCV를 이용하여 최적으로 학습된 estimator로 predict 수행. \n",
        "gb_pred = grid_cv.best_estimator_.predict(X_test)\n",
        "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
        "print('GBM 정확도: {0:.4f}'.format(gb_accuracy))"
      ],
      "metadata": {
        "id": "5c9Kvu7Fv3pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LightGBM의 파이썬 패키지인 lightgbm에서 LGBMClassifier 임포트\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = load_breast_cancer()\n",
        "\n",
        "cancer_df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
        "cancer_df['target']= dataset.target\n",
        "X_features = cancer_df.iloc[:, :-1]\n",
        "y_label = cancer_df.iloc[:, -1]\n",
        "\n",
        "# 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출\n",
        "X_train, X_test, y_train, y_test=train_test_split(X_features, y_label, test_size=0.2, random_state=156 )\n",
        "\n",
        "# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검증용 데이터로 분리\n",
        "X_tr, X_val, y_tr, y_val= train_test_split(X_train, y_train, test_size=0.1, random_state=156 )\n",
        "\n",
        "# 앞서 XGBoost와 동일하게 n_estimators는 400 설정.\n",
        "lgbm_wrapper = LGBMClassifier(n_estimators=400, learning_rate=0.05)\n",
        "\n",
        "# LightGBM도 XGBoost와 동일하게 조기 중단 수행 가능.\n",
        "evals = [(X_tr, y_tr), (X_val, y_val)]\n",
        "lgbm_wrapper.fit(X_tr, y_tr, early_stopping_rounds=50, eval_metric=\"logloss\", eval_set=evals, verbose=True)\n",
        "preds = lgbm_wrapper.predict(X_test)\n",
        "pred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1]\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
        "    confusion = confusion_matrix( y_test, pred)\n",
        "    accuracy = accuracy_score(y_test , pred)\n",
        "    precision = precision_score(y_test , pred)\n",
        "    recall = recall_score(y_test , pred)\n",
        "    f1 = f1_score(y_test,pred)\n",
        "    # ROC-AUC 추가 \n",
        "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
        "    print('오차 행렬')\n",
        "    print(confusion)\n",
        "    # ROC-AUC print 추가\n",
        "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
        "    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))\n",
        "get_clf_eval(y_test, preds, pred_proba)\n",
        "# plot_importance( )를 이용하여 feature 중요도 시각화\n",
        "from lightgbm import plot_importance\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 12))\n",
        "plot_importance(lgbm_wrapper, ax=ax)\n",
        "plt.savefig('lightgbm_feature_importance.tif', format='tif', dpi=300, bbox_inches='tight')"
      ],
      "metadata": {
        "id": "9kmz4qbzv9Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hyperopt\n",
        "\n",
        "print(hyperopt.__version__)\n",
        "from hyperopt import hp\n",
        "\n",
        "# -10 ~ 10까지 1간격을 가지는 입력 변수 x와 -15 ~ 15까지 1간격으로 입력 변수 y 설정.\n",
        "search_space = {'x': hp.quniform('x', -10, 10, 1), 'y': hp.quniform('y', -15, 15, 1) }\n",
        "from hyperopt import STATUS_OK\n",
        "\n",
        "# 목적 함수를 생성. 변숫값과 변수 검색 공간을 가지는 딕셔너리를 인자로 받고, 특정 값을 반환\n",
        "def objective_func(search_space):\n",
        "    x = search_space['x']\n",
        "    y = search_space['y']\n",
        "    retval = x**2 - 20*y\n",
        "    \n",
        "    return retval\n",
        "from hyperopt import fmin, tpe, Trials\n",
        "import numpy as np\n",
        "\n",
        "# 입력 결괏값을 저장한 Trials 객체값 생성.\n",
        "trial_val = Trials()\n",
        "\n",
        "# 목적 함수의 최솟값을 반환하는 최적 입력 변숫값을 5번의 입력값 시도(max_evals=5)로 찾아냄.\n",
        "best_01 = fmin(fn=objective_func, space=search_space, algo=tpe.suggest, max_evals=5\n",
        "               , trials=trial_val, rstate=np.random.default_rng(seed=0))\n",
        "print('best:', best_01)\n",
        "trial_val = Trials()\n",
        "\n",
        "# max_evals를 20회로 늘려서 재테스트\n",
        "best_02 = fmin(fn=objective_func, space=search_space, algo=tpe.suggest, max_evals=20\n",
        "               , trials=trial_val, rstate=np.random.default_rng(seed=0))\n",
        "print('best:', best_02)\n",
        "# fmin( )에 인자로 들어가는 Trials 객체의 result 속성에 파이썬 리스트로 목적 함수 반환값들이 저장됨\n",
        "# 리스트 내부의 개별 원소는 {'loss':함수 반환값, 'status':반환 상태값} 와 같은 딕셔너리임. \n",
        "print(trial_val.results)\n",
        "# Trials 객체의 vals 속성에 {'입력변수명':개별 수행 시마다 입력된 값 리스트} 형태로 저장됨.\n",
        "print(trial_val.vals)\n",
        "import pandas as pd\n",
        "\n",
        "# results에서 loss 키값에 해당하는 밸류들을 추출하여 list로 생성. \n",
        "losses = [loss_dict['loss'] for loss_dict in trial_val.results]\n",
        "\n",
        "# DataFrame으로 생성.\n",
        "result_df = pd.DataFrame({'x': trial_val.vals['x'], 'y': trial_val.vals['y'], 'losses': losses})\n",
        "result_df"
      ],
      "metadata": {
        "id": "f7uoFad_UUWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "dataset = load_breast_cancer()\n",
        "\n",
        "cancer_df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
        "cancer_df['target']= dataset.target\n",
        "X_features = cancer_df.iloc[:, :-1]\n",
        "y_label = cancer_df.iloc[:, -1]\n",
        "# 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출\n",
        "X_train, X_test, y_train, y_test=train_test_split(X_features, y_label, test_size=0.2, random_state=156 )\n",
        "\n",
        "# 앞에서 추출한 학습 데이터를 다시 학습과 검증 데이터로 분리\n",
        "X_tr, X_val, y_tr, y_val= train_test_split(X_train, y_train, test_size=0.1, random_state=156 )\n",
        "from hyperopt import hp\n",
        "\n",
        "# max_depth는 5에서 20까지 1간격으로, min_child_weight는 1에서 2까지 1간격으로\n",
        "# colsample_bytree는 0.5에서 1사이, learning_rate는 0.01에서 0.2 사이 정규 분포된 값으로 검색.\n",
        "xgb_search_space = {'max_depth': hp.quniform('max_depth', 5, 20, 1), \n",
        "                    'min_child_weight': hp.quniform('min_child_weight', 1, 2, 1),\n",
        "                    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
        "                    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
        "                   }\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from xgboost import XGBClassifier\n",
        "from hyperopt import STATUS_OK\n",
        "\n",
        "# fmin()에서 입력된 search_space 값으로 입력된 모든 값은 실수형임.\n",
        "# XGBClassifier의 정수형 하이퍼 파라미터는 정수형 변환을 해줘야 함.\n",
        "# 정확도는 높을수록 더 좋은 수치임. -1 * 정확도를 곱해서 큰 정확도 값일수록 최소가 되도록 변환\n",
        "def objective_func(search_space):\n",
        "    # 수행 시간 절약을 위해 nestimators는 100으로 축소\n",
        "    xgb_clf = XGBClassifier(n_estimators=100, max_depth=int(search_space['max_depth']),\n",
        "                            min_child_weight=int(search_space['min_child_weight']),\n",
        "                            learning_rate=search_space['learning_rate'],\n",
        "                            colsample_bytree=search_space['colsample_bytree'],\n",
        "                            eval_metric='logloss')\n",
        "    accuracy = cross_val_score(xgb_clf, X_train, y_train, scoring='accuracy', cv=3)\n",
        "    \n",
        "    # accuracy는 cv=3 개수만큼 roc-auc 결과를 리스트로 가짐. 이를 평균해서 반환하되 -1을 곱함.\n",
        "    return {'loss':-1 * np.mean(accuracy), 'status': STATUS_OK}\n",
        "from hyperopt import fmin, tpe, Trials\n",
        "\n",
        "trial_val = Trials()\n",
        "best = fmin(fn=objective_func,\n",
        "            space=xgb_search_space,\n",
        "            algo=tpe.suggest,\n",
        "            max_evals=50, # 최대 반복 횟수를 지정합니다.\n",
        "            trials=trial_val, rstate=np.random.default_rng(seed=9))\n",
        "print('best:', best)\n",
        "print('colsample_bytree:{0}, learning_rate:{1}, max_depth:{2}, min_child_weight:{3}'.format(\n",
        "    round(best['colsample_bytree'], 5), round(best['learning_rate'], 5),\n",
        "    int(best['max_depth']), int(best['min_child_weight'])))\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
        "    confusion = confusion_matrix( y_test, pred)\n",
        "    accuracy = accuracy_score(y_test , pred)\n",
        "    precision = precision_score(y_test , pred)\n",
        "    recall = recall_score(y_test , pred)\n",
        "    f1 = f1_score(y_test,pred)\n",
        "    # ROC-AUC 추가 \n",
        "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
        "    print('오차 행렬')\n",
        "    print(confusion)\n",
        "    # ROC-AUC print 추가\n",
        "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
        "    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))\n",
        "xgb_wrapper = XGBClassifier(n_estimators=400,\n",
        "                            learning_rate=round(best['learning_rate'], 5),\n",
        "                            max_depth=int(best['max_depth']),\n",
        "                            min_child_weight=int(best['min_child_weight']),\n",
        "                            colsample_bytree=round(best['colsample_bytree'], 5)\n",
        "                           )\n",
        "\n",
        "evals = [(X_tr, y_tr), (X_val, y_val)]\n",
        "xgb_wrapper.fit(X_tr, y_tr, early_stopping_rounds=50, eval_metric='logloss',\n",
        "                eval_set=evals, verbose=True)\n",
        "\n",
        "preds = xgb_wrapper.predict(X_test)\n",
        "pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]\n",
        "\n",
        "get_clf_eval(y_test, preds, pred_proba)"
      ],
      "metadata": {
        "id": "zM3OSWVlUXqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "cust_df = pd.read_csv(\"./train_santander.csv\", encoding='latin-1')\n",
        "print('dataset shape:', cust_df.shape)\n",
        "cust_df.head(3)"
      ],
      "metadata": {
        "id": "s-6TLh3vUYb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# var3 피처 값 대체 및 ID 피처 드롭\n",
        "cust_df['var3'].replace(-999999, 2, inplace=True)\n",
        "cust_df.drop('ID', axis=1, inplace=True)\n",
        "\n",
        "# 피처 세트와 레이블 세트분리. 레이블 컬럼은 DataFrame의 맨 마지막에 위치해 컬럼 위치 -1로 분리\n",
        "X_features = cust_df.iloc[:, :-1]\n",
        "y_labels = cust_df.iloc[:, -1]\n",
        "print('피처 데이터 shape:{0}'.format(X_features.shape))"
      ],
      "metadata": {
        "id": "2LVWpDykUvxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels,\n",
        "                                                    test_size=0.2, random_state=0)\n",
        "train_cnt = y_train.count()\n",
        "test_cnt = y_test.count()\n",
        "print('학습 세트 Shape:{0}, 테스트 세트 Shape:{1}'.format(X_train.shape , X_test.shape))\n",
        "\n",
        "print(' 학습 세트 레이블 값 분포 비율')\n",
        "print(y_train.value_counts()/train_cnt)\n",
        "print('\\n 테스트 세트 레이블 값 분포 비율')\n",
        "print(y_test.value_counts()/test_cnt)"
      ],
      "metadata": {
        "id": "6Icsn2IgUzP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# n_estimators는 500으로, learning_rate 0.05, random state는 예제 수행 시마다 동일 예측 결과를 위해 설정. \n",
        "xgb_clf = XGBClassifier(n_estimators=500, learning_rate=0.05, random_state=156)\n",
        "\n",
        "# 성능 평가 지표를 auc로, 조기 중단 파라미터는 100으로 설정하고 학습 수행. \n",
        "xgb_clf.fit(X_tr, y_tr, early_stopping_rounds=100, eval_metric='auc', eval_set=[(X_tr, y_tr), (X_val, y_val)])\n",
        "\n",
        "xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, 1])\n",
        "print('ROC AUC: {0:.4f}'.format(xgb_roc_score))"
      ],
      "metadata": {
        "id": "RDJWvV1hU1-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# 목적 함수 설정. \n",
        "# 추후 fmin()에서 입력된 search_space값으로 XGBClassifier 교차 검증 학습 후 -1* roc_auc 평균 값을 반환.  \n",
        "def objective_func(search_space):\n",
        "    xgb_clf = XGBClassifier(n_estimators=100, max_depth=int(search_space['max_depth']),\n",
        "                            min_child_weight=int(search_space['min_child_weight']),\n",
        "                            colsample_bytree=search_space['colsample_bytree'],\n",
        "                            learning_rate=search_space['learning_rate']\n",
        "                           )\n",
        "    # 3개 k-fold 방식으로 평가된 roc_auc 지표를 담는 list\n",
        "    roc_auc_list= []\n",
        "    \n",
        "    # 3개 k-fold방식 적용 \n",
        "    kf = KFold(n_splits=3)\n",
        "    # X_train을 다시 학습과 검증용 데이터로 분리\n",
        "    for tr_index, val_index in kf.split(X_train):\n",
        "        # kf.split(X_train)으로 추출된 학습과 검증 index값으로 학습과 검증 데이터 세트 분리 \n",
        "        X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]\n",
        "        X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
        "        # early stopping은 30회로 설정하고 추출된 학습과 검증 데이터로 XGBClassifier 학습 수행. \n",
        "        xgb_clf.fit(X_tr, y_tr, early_stopping_rounds=30, eval_metric='auc',\n",
        "                   eval_set=[(X_tr, y_tr), (X_val, y_val)])\n",
        "    \n",
        "        # 1로 예측한 확률값 추출후 roc auc 계산하고 평균 roc auc 계산을 위해 list에 결과값 담음. \n",
        "        score = roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:, 1])\n",
        "        roc_auc_list.append(score)\n",
        "        \n",
        "    # 3개 k-fold로 계산된 roc_auc값의 평균값을 반환하되, \n",
        "    # HyperOpt는 목적함수의 최소값을 위한 입력값을 찾으므로 -1을 곱한 뒤 반환. \n",
        "    return -1 * np.mean(roc_auc_list)\n",
        "from hyperopt import fmin, tpe, Trials\n",
        "\n",
        "trials = Trials()\n",
        "\n",
        "# fmin()함수를 호출. max_evals지정된 횟수만큼 반복 후 목적함수의 최소값을 가지는 최적 입력값 추출.\n",
        "best = fmin(fn=objective_func,\n",
        "            space=xgb_search_space,\n",
        "            algo=tpe.suggest,\n",
        "            max_evals=50, # 최대 반복 횟수를 지정합니다.\n",
        "            trials=trials, rstate=np.random.default_rng(seed=30))\n",
        "\n",
        "print('best:', best)"
      ],
      "metadata": {
        "id": "cqBa5XxOU9Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline\n",
        "\n",
        "card_df = pd.read_csv('./creditcard.csv')\n",
        "card_df.head(3)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 인자로 입력받은 DataFrame을 복사 한 뒤 Time 컬럼만 삭제하고 복사된 DataFrame 반환\n",
        "def get_preprocessed_df(df=None):\n",
        "    df_copy = df.copy()\n",
        "    df_copy.drop('Time', axis=1, inplace=True)\n",
        "    return df_copy\n",
        "# 사전 데이터 가공 후 학습과 테스트 데이터 세트를 반환하는 함수.\n",
        "def get_train_test_dataset(df=None):\n",
        "    # 인자로 입력된 DataFrame의 사전 데이터 가공이 완료된 복사 DataFrame 반환\n",
        "    df_copy = get_preprocessed_df(df)\n",
        "    # DataFrame의 맨 마지막 컬럼이 레이블, 나머지는 피처들\n",
        "    X_features = df_copy.iloc[:, :-1]\n",
        "    y_target = df_copy.iloc[:, -1]\n",
        "    # train_test_split( )으로 학습과 테스트 데이터 분할. stratify=y_target으로 Stratified 기반 분할\n",
        "    X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(X_features, y_target, test_size=0.3, random_state=0, stratify=y_target)\n",
        "    # 학습과 테스트 데이터 세트 반환\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n",
        "print('학습 데이터 레이블 값 비율')\n",
        "print(y_train.value_counts()/y_train.shape[0] * 100)\n",
        "print('테스트 데이터 레이블 값 비율')\n",
        "print(y_test.value_counts()/y_test.shape[0] * 100)\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
        "    confusion = confusion_matrix( y_test, pred)\n",
        "    accuracy = accuracy_score(y_test , pred)\n",
        "    precision = precision_score(y_test , pred)\n",
        "    recall = recall_score(y_test , pred)\n",
        "    f1 = f1_score(y_test,pred)\n",
        "    # ROC-AUC 추가 \n",
        "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
        "    print('오차 행렬')\n",
        "    print(confusion)\n",
        "    # ROC-AUC print 추가\n",
        "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
        "    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(X_train, y_train)\n",
        "lr_pred = lr_clf.predict(X_test)\n",
        "lr_pred_proba = lr_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 3장에서 사용한 get_clf_eval() 함수를 이용하여 평가 수행. \n",
        "get_clf_eval(y_test, lr_pred, lr_pred_proba)\n",
        "# 인자로 사이킷런의 Estimator객체와, 학습/테스트 데이터 세트를 입력 받아서 학습/예측/평가 수행.\n",
        "def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None):\n",
        "    model.fit(ftr_train, tgt_train)\n",
        "    pred = model.predict(ftr_test)\n",
        "    pred_proba = model.predict_proba(ftr_test)[:, 1]\n",
        "    get_clf_eval(tgt_test, pred, pred_proba)\n",
        "    \n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
        "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"
      ],
      "metadata": {
        "id": "YIvUZuVEVCPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.xticks(range(0, 30000, 1000), rotation=60)\n",
        "sns.histplot(card_df['Amount'], bins=100, kde=True)\n",
        "plt.show()\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# 사이킷런의 StandardScaler를 이용하여 정규분포 형태로 Amount 피처값 변환하는 로직으로 수정. \n",
        "def get_preprocessed_df(df=None):\n",
        "    df_copy = df.copy()\n",
        "    scaler = StandardScaler()\n",
        "    amount_n = scaler.fit_transform(df_copy['Amount'].values.reshape(-1, 1))\n",
        "    # 변환된 Amount를 Amount_Scaled로 피처명 변경후 DataFrame맨 앞 컬럼으로 입력\n",
        "    df_copy.insert(0, 'Amount_Scaled', amount_n)\n",
        "    # 기존 Time, Amount 피처 삭제\n",
        "    df_copy.drop(['Time','Amount'], axis=1, inplace=True)\n",
        "    return df_copy\n",
        "# Amount를 정규분포 형태로 변환 후 로지스틱 회귀 및 LightGBM 수행. \n",
        "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n",
        "\n",
        "print('### 로지스틱 회귀 예측 성능 ###')\n",
        "lr_clf = LogisticRegression()\n",
        "get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n",
        "\n",
        "print('### LightGBM 예측 성능 ###')\n",
        "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
        "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n",
        "def get_preprocessed_df(df=None):\n",
        "    df_copy = df.copy()\n",
        "    # 넘파이의 log1p( )를 이용하여 Amount를 로그 변환 \n",
        "    amount_n = np.log1p(df_copy['Amount'])\n",
        "    df_copy.insert(0, 'Amount_Scaled', amount_n)\n",
        "    df_copy.drop(['Time','Amount'], axis=1, inplace=True)\n",
        "    return df_copy\n",
        "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n",
        "\n",
        "print('### 로지스틱 회귀 예측 성능 ###')\n",
        "get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n",
        "\n",
        "print('### LightGBM 예측 성능 ###')\n",
        "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"
      ],
      "metadata": {
        "id": "FhOO1j0cVE_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "cancer_data = load_breast_cancer()\n",
        "\n",
        "X_data = cancer_data.data\n",
        "y_label = cancer_data.target\n",
        "\n",
        "X_train , X_test , y_train , y_test = train_test_split(X_data , y_label , test_size=0.2 , random_state=0)\n",
        "# 개별 ML 모델을 위한 Classifier 생성.\n",
        "knn_clf  = KNeighborsClassifier(n_neighbors=4)\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "ada_clf = AdaBoostClassifier(n_estimators=100)\n",
        "\n",
        "# 최종 Stacking 모델을 위한 Classifier생성. \n",
        "lr_final = LogisticRegression(C=10)\n",
        "# 개별 모델들을 학습. \n",
        "knn_clf.fit(X_train, y_train)\n",
        "rf_clf.fit(X_train , y_train)\n",
        "dt_clf.fit(X_train , y_train)\n",
        "ada_clf.fit(X_train, y_train)\n",
        "# 학습된 개별 모델들이 각자 반환하는 예측 데이터 셋을 생성하고 개별 모델의 정확도 측정. \n",
        "knn_pred = knn_clf.predict(X_test)\n",
        "rf_pred = rf_clf.predict(X_test)\n",
        "dt_pred = dt_clf.predict(X_test)\n",
        "ada_pred = ada_clf.predict(X_test)\n",
        "\n",
        "print('KNN 정확도: {0:.4f}'.format(accuracy_score(y_test, knn_pred)))\n",
        "print('랜덤 포레스트 정확도: {0:.4f}'.format(accuracy_score(y_test, rf_pred)))\n",
        "print('결정 트리 정확도: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))\n",
        "print('에이다부스트 정확도: {0:.4f} :'.format(accuracy_score(y_test, ada_pred)))\n",
        "pred = np.array([knn_pred, rf_pred, dt_pred, ada_pred])\n",
        "print(pred.shape)\n",
        "\n",
        "# transpose를 이용해 행과 열의 위치 교환. 컬럼 레벨로 각 알고리즘의 예측 결과를 피처로 만듦. \n",
        "pred = np.transpose(pred)\n",
        "print(pred.shape)\n",
        "lr_final.fit(pred, y_test)\n",
        "final = lr_final.predict(pred)\n",
        "\n",
        "print('최종 메타 모델의 예측 정확도: {0:.4f}'.format(accuracy_score(y_test , final)))"
      ],
      "metadata": {
        "id": "Pdk0a3uXVIi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# 개별 기반 모델에서 최종 메타 모델이 사용할 학습 및 테스트용 데이터를 생성하기 위한 함수. \n",
        "def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds ):\n",
        "    # 지정된 n_folds값으로 KFold 생성.\n",
        "    kf = KFold(n_splits=n_folds, shuffle=False)\n",
        "    #추후에 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화 \n",
        "    train_fold_pred = np.zeros((X_train_n.shape[0] ,1 ))\n",
        "    test_pred = np.zeros((X_test_n.shape[0],n_folds))\n",
        "    print(model.__class__.__name__ , ' model 시작 ')\n",
        "    \n",
        "    for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train_n)):\n",
        "        #입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 셋 추출 \n",
        "        print('\\t 폴드 세트: ',folder_counter,' 시작 ')\n",
        "        X_tr = X_train_n[train_index] \n",
        "        y_tr = y_train_n[train_index] \n",
        "        X_te = X_train_n[valid_index]  \n",
        "        \n",
        "        #폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행.\n",
        "        model.fit(X_tr , y_tr)       \n",
        "        #폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장.\n",
        "        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1)\n",
        "        #입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장. \n",
        "        test_pred[:, folder_counter] = model.predict(X_test_n)\n",
        "            \n",
        "    # 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성 \n",
        "    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1)    \n",
        "    \n",
        "    #train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터\n",
        "    return train_fold_pred , test_pred_mean\n",
        "knn_train, knn_test = get_stacking_base_datasets(knn_clf, X_train, y_train, X_test, 7)\n",
        "rf_train, rf_test = get_stacking_base_datasets(rf_clf, X_train, y_train, X_test, 7)\n",
        "dt_train, dt_test = get_stacking_base_datasets(dt_clf, X_train, y_train, X_test,  7)    \n",
        "ada_train, ada_test = get_stacking_base_datasets(ada_clf, X_train, y_train, X_test, 7)\n",
        "Stack_final_X_train = np.concatenate((knn_train, rf_train, dt_train, ada_train), axis=1)\n",
        "Stack_final_X_test = np.concatenate((knn_test, rf_test, dt_test, ada_test), axis=1)\n",
        "print('원본 학습 피처 데이터 Shape:',X_train.shape, '원본 테스트 피처 Shape:',X_test.shape)\n",
        "print('스태킹 학습 피처 데이터 Shape:', Stack_final_X_train.shape,\n",
        "      '스태킹 테스트 피처 데이터 Shape:',Stack_final_X_test.shape)\n",
        "lr_final.fit(Stack_final_X_train, y_train)\n",
        "stack_final = lr_final.predict(Stack_final_X_test)\n",
        "\n",
        "print('최종 메타 모델의 예측 정확도: {0:.4f}'.format(accuracy_score(y_test, stack_final)))"
      ],
      "metadata": {
        "id": "J3yMmLUSVOko"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}