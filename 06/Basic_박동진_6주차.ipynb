{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# y = 4X + 6 식을 근사(w1 = 4, w0 = 6). random 값은 Noise를 위해 만듬\n",
    "X = 2 * np.random.rand(100,1)\n",
    "y = 6 + 4 * X + np.random.randn(100,1)\n",
    "\n",
    "# X, y 데이터 셋 scatter plot으로 시각화\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 오차 비용을 계산하는 함수 생성\n",
    "def get_cost(y, y_pred):\n",
    "    N = len(y) \n",
    "    cost = np.sum(np.square(y - y_pred))/N\n",
    "    return cost\n",
    "# w1 과 w0 를 업데이트 할 w1_update, w0_update를 반환. \n",
    "def get_weight_updates(w1, w0, X, y, learning_rate=0.01):\n",
    "    N = len(y)\n",
    "    \n",
    "    # 먼저 w1_update, w0_update를 각각 w1, w0의 shape와 동일한 크기를 가진 0 값으로 초기화\n",
    "    w1_update = np.zeros_like(w1)\n",
    "    w0_update = np.zeros_like(w0)\n",
    "    \n",
    "    # 예측 배열 계산하고 예측과 실제 값의 차이 계산\n",
    "    y_pred = np.dot(X, w1.T) + w0\n",
    "    diff = y - y_pred\n",
    "         \n",
    "    # w0_update를 dot 행렬 연산으로 구하기 위해 모두 1값을 가진 행렬 생성 \n",
    "    w0_factors = np.ones((N,1))\n",
    "\n",
    "    # w1과 w0을 업데이트할 w1_update와 w0_update 계산\n",
    "    w1_update = -(2/N)*learning_rate*(np.dot(X.T, diff))\n",
    "    w0_update = -(2/N)*learning_rate*(np.dot(w0_factors.T, diff))    \n",
    "    \n",
    "    return w1_update, w0_update\n",
    "# 입력 인자 iters로 주어진 횟수만큼 반복적으로 w1과 w0를 업데이트 적용함. \n",
    "def gradient_descent_steps(X, y, iters=10000):\n",
    "    # w0와 w1을 모두 0으로 초기화. \n",
    "    w0 = np.zeros((1,1))\n",
    "    w1 = np.zeros((1,1))\n",
    "    \n",
    "    # 인자로 주어진 iters 만큼 반복적으로 get_weight_updates() 호출하여 w1, w0 업데이트 수행. \n",
    "    for ind in range(iters):\n",
    "        w1_update, w0_update = get_weight_updates(w1, w0, X, y, learning_rate=0.01)\n",
    "        w1 = w1 - w1_update\n",
    "        w0 = w0 - w0_update\n",
    "              \n",
    "    return w1, w0\n",
    "w1, w0 = gradient_descent_steps(X, y, iters=1000)\n",
    "print(\"w1:{0:.3f} w0:{1:.3f}\".format(w1[0,0], w0[0,0]))\n",
    "y_pred = w1[0,0] * X + w0\n",
    "print('Gradient Descent Total Cost:{0:.4f}'.format(get_cost(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.plot(X, y_pred, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미니 배치 확률적 경사 하강법을 이용한 최적 비용함수 도출\n",
    "def stochastic_gradient_descent_steps(X, y, batch_size=10, iters=1000):\n",
    "    w0 = np.zeros((1,1))\n",
    "    w1 = np.zeros((1,1))\n",
    "    prev_cost = 100000\n",
    "    iter_index =0\n",
    "    \n",
    "    for ind in range(iters):\n",
    "        np.random.seed(ind)\n",
    "        \n",
    "        # 전체 X, y 데이터에서 랜덤하게 batch_size만큼 데이터 추출하여 sample_X, sample_y로 저장\n",
    "        stochastic_random_index = np.random.permutation(X.shape[0])\n",
    "        sample_X = X[stochastic_random_index[0:batch_size]]\n",
    "        sample_y = y[stochastic_random_index[0:batch_size]]\n",
    "        \n",
    "        # 랜덤하게 batch_size만큼 추출된 데이터 기반으로 w1_update, w0_update 계산 후 업데이트\n",
    "        w1_update, w0_update = get_weight_updates(w1, w0, sample_X, sample_y, learning_rate=0.01)\n",
    "        w1 = w1 - w1_update\n",
    "        w0 = w0 - w0_update\n",
    "    \n",
    "    return w1, w0\n",
    "w1, w0 = stochastic_gradient_descent_steps(X, y, iters=1000)\n",
    "print(\"w1:\",round(w1[0,0],3),\"w0:\",round(w0[0,0],3))\n",
    "y_pred = w1[0,0] * X + w0\n",
    "print('Stochastic Gradient Descent Total Cost:{0:.4f}'.format(get_cost(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.plot(X, y_pred, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  #사이킷런 1.2 부터는 보스턴 주택가격 데이터가 없어진다는 warning 메시지 출력 제거\n",
    "%matplotlib inline\n",
    "\n",
    "# boston 데이타셋 로드\n",
    "bostonDF = pd.read_csv('./dataset/BostonHousing.csv')\n",
    "\n",
    "# boston dataset의 target array는 주택 가격임. medv를 PRICE 컬럼으로 DataFrame에 추가함\n",
    "bostonDF.rename(columns={'medv':'PRICE'}, inplace=True)\n",
    "\n",
    "bostonDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2개의 행과 4개의 열을 가진 subplots를 이용. axs는 4x2개의 ax를 가짐.\n",
    "fig, axs = plt.subplots(figsize=(16,8) , ncols=4 , nrows=2)\n",
    "lm_features = ['rm','zn','indus','nox','age','ptratio','lstat','rad']\n",
    "for i , feature in enumerate(lm_features):\n",
    "    row = int(i/4)\n",
    "    col = i%4\n",
    "    # 시본의 regplot을 이용해 산점도와 선형 회귀 직선을 함께 표현\n",
    "    sns.regplot(x=feature , y='PRICE',data=bostonDF , ax=axs[row][col])\n",
    "\n",
    "fig1 = plt.gcf()\n",
    "fig1.savefig('p322_boston.tif', format='tif', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "y_target = bostonDF['PRICE']\n",
    "X_data = bostonDF.drop(['PRICE'],axis=1,inplace=False)\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(X_data , y_target ,test_size=0.3, random_state=156)\n",
    "\n",
    "# Linear Regression OLS로 학습/예측/평가 수행. \n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train ,y_train )\n",
    "y_preds = lr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_preds)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print('MSE : {0:.3f} , RMSE : {1:.3F}'.format(mse , rmse))\n",
    "print('Variance score : {0:.3f}'.format(r2_score(y_test, y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('절편 값:',lr.intercept_)\n",
    "print('회귀 계수값:', np.round(lr.coef_, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회귀 계수를 큰 값 순으로 정렬하기 위해 Series로 생성. index가 칼럼명에 유의\n",
    "coeff = pd.Series(data=np.round(lr.coef_, 1), index=X_data.columns )\n",
    "coeff.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "y_target = bostonDF['PRICE']\n",
    "X_data = bostonDF.drop(['PRICE'],axis=1,inplace=False)\n",
    "lr = LinearRegression()\n",
    "\n",
    "# cross_val_score( )로 5 Fold 셋으로 MSE 를 구한 뒤 이를 기반으로 다시  RMSE 구함. \n",
    "neg_mse_scores = cross_val_score(lr, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5)\n",
    "rmse_scores  = np.sqrt(-1 * neg_mse_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "\n",
    "# cross_val_score(scoring=\"neg_mean_squared_error\")로 반환된 값은 모두 음수 \n",
    "print(' 5 folds 의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 2))\n",
    "print(' 5 folds 의 개별 RMSE scores : ', np.round(rmse_scores, 2))\n",
    "print(' 5 folds 의 평균 RMSE : {0:.3f} '.format(avg_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_func(X):\n",
    "    y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3\n",
    "    print(X[:, 0])\n",
    "    print(X[:, 1])\n",
    "    return y\n",
    "\n",
    "X = np.arange(0,4).reshape(2,2)\n",
    "\n",
    "print('일차 단항식 계수 feature: \\n' ,X)\n",
    "y = polynomial_func(X)\n",
    "print('삼차 다항식 결정값: \\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3차 다항식 변환\n",
    "# [x_1, x_2] -> [1, x_1, x_2, x_1^2, x_1*x_2, x_2^2, x_1^3, x_1^2*x_2, x_1*x_2^2, x_2^3b]\n",
    "poly_ftr = PolynomialFeatures(degree=3).fit_transform(X)\n",
    "print('3차 다항식 계수 feature: \\n',poly_ftr)\n",
    "\n",
    "# Linear Regression에 3차 다항식 계수 feature와 3차 다항식 결정값으로 학습 후 회귀 계수 확인\n",
    "model = LinearRegression()\n",
    "model.fit(poly_ftr,y)\n",
    "print('Polynomial 회귀 계수\\n' , np.round(model.coef_, 2))\n",
    "print('Polynomial 회귀 Shape :', model.coef_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "def polynomial_func(X):\n",
    "    y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3 \n",
    "    return y\n",
    "\n",
    "# Pipeline 객체로 Streamline 하게 Polynomial Feature변환과 Linear Regression을 연결\n",
    "model = Pipeline([('poly', PolynomialFeatures(degree=3)),\n",
    "                  ('linear', LinearRegression())])\n",
    "X = np.arange(4).reshape(2,2)\n",
    "y = polynomial_func(X)\n",
    "\n",
    "model = model.fit(X, y)\n",
    "print('Polynomial 회귀 계수\\n', np.round(model.named_steps['linear'].coef_, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "%matplotlib inline\n",
    "\n",
    "# 임의의 값으로 구성된 X값에 대해 코사인 변환 값을 반환.\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "# X는 0부터 1까지 30개의 임의의 값을 순서대로 샘플링한 데이터입니다.\n",
    "np.random.seed(0)\n",
    "n_samples = 30\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "\n",
    "# y 값은 코사인 기반의 true_fun()에서 약간의 노이즈 변동 값을 더한 값입니다.\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "degrees = [1, 4, 15]\n",
    "\n",
    "# 다항 회귀의 차수(degree)를 1, 4, 15로 각각 변화시키면서 비교합니다.\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "    \n",
    "    # 개별 degree별로 Polynomial 변환합니다.\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X.reshape(-1, 1), y)\n",
    "    \n",
    "    # 교차 검증으로 다항 회귀를 평가합니다.\n",
    "    scores = cross_val_score(pipeline, X.reshape(-1, 1), y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    \n",
    "    # Pipeline을 구성하는 세부 객체를 접근하는 named_steps['객체명']을 이용해 회귀계수 추출\n",
    "    coefficients = pipeline.named_steps['linear_regression'].coef_\n",
    "    print('\\nDegree {0} 회귀 계수는 {1} 입니다.'.format(degrees[i], np.round(coefficients, 2)))\n",
    "    print('Degree {0} MSE 는 {1} 입니다.'.format(degrees[i], -1*np.mean(scores)))\n",
    "          \n",
    "    # 0 부터 1까지 테스트 데이터 세트를 100개로 나눠 예측을 수행합니다.\n",
    "    # 테스트 데이터 세트에 회귀 예측을 수행하고 예측 곡선과 실제 곡선을 그려서 비교합니다.\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "    \n",
    "    # 예측값 곡선\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "    \n",
    "    # 실제 값 곡선\n",
    "    plt.plot(X_test, true_fun(X_test), '--', label=\"True function\")\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.xlim((0, 1)); plt.ylim((-2, 2)); plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(degrees[i], -scores.mean(), scores.std()))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞의 LinearRegression예제에서 분할한 feature 데이터 셋인 X_data과 Target 데이터 셋인 Y_target 데이터셋을 그대로 이용 \n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# boston 데이타셋 로드\n",
    "bostonDF= pd.read_csv('./dataset/BostonHousing.csv')\n",
    "\n",
    "# boston dataset의 target array는 주택 가격임. medv를 PRICE 컬럼으로 DataFrame에 추가함\n",
    "bostonDF.rename(columns={'medv':'PRICE'}, inplace=True)\n",
    "\n",
    "y_target = bostonDF['PRICE']\n",
    "X_data = bostonDF.drop(['PRICE'],axis=1,inplace=False)\n",
    "\n",
    "\n",
    "ridge = Ridge(alpha = 10)\n",
    "neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5)\n",
    "rmse_scores  = np.sqrt(-1 * neg_mse_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "print(' 5 folds 의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 3))\n",
    "print(' 5 folds 의 개별 RMSE scores : ', np.round(rmse_scores,3))\n",
    "print(' 5 folds 의 평균 RMSE : {0:.3f} '.format(avg_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 릿지에 사용될 alpha 파라미터의 값을 정의\n",
    "alphas = [0, 0.1, 1, 10, 100]\n",
    "\n",
    "# alphas list 값을 반복하면서 alpha에 따른 평균 rmse를 구함.\n",
    "for alpha in alphas :\n",
    "    ridge = Ridge(alpha = alpha)\n",
    "    \n",
    "    # cross_val_score를 이용해 5 폴드의 평균 RMSE를 계산\n",
    "    neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5)\n",
    "    avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))\n",
    "    print('alpha {0} 일 때 5 folds 의 평균 RMSE : {1:.3f} '.format(alpha, avg_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 alpha에 따른 회귀 계수 값을 시각화하기 위해 5개의 열로 된 맷플롯립 축 생성  \n",
    "fig , axs = plt.subplots(figsize=(18,6) , nrows=1 , ncols=5)\n",
    "# 각 alpha에 따른 회귀 계수 값을 데이터로 저장하기 위한 DataFrame 생성  \n",
    "coeff_df = pd.DataFrame()\n",
    "\n",
    "# alphas 리스트 값을 차례로 입력해 회귀 계수 값 시각화 및 데이터 저장. pos는 axis의 위치 지정\n",
    "for pos , alpha in enumerate(alphas) :\n",
    "    ridge = Ridge(alpha = alpha)\n",
    "    ridge.fit(X_data , y_target)\n",
    "    # alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame의 컬럼으로 추가.  \n",
    "    coeff = pd.Series(data=ridge.coef_ , index=X_data.columns )\n",
    "    colname='alpha:'+str(alpha)\n",
    "    coeff_df[colname] = coeff\n",
    "    # 막대 그래프로 각 alpha 값에서의 회귀 계수를 시각화. 회귀 계수값이 높은 순으로 표현\n",
    "    coeff = coeff.sort_values(ascending=False)\n",
    "    axs[pos].set_title(colname)\n",
    "    axs[pos].set_xlim(-3,6)\n",
    "    sns.barplot(x=coeff.values , y=coeff.index, ax=axs[pos])\n",
    "\n",
    "# for 문 바깥에서 맷플롯립의 show 호출 및 alpha에 따른 피처별 회귀 계수를 DataFrame으로 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_alphas = [0 , 0.1 , 1 , 10 , 100]\n",
    "sort_column = 'alpha:'+str(ridge_alphas[0])\n",
    "coeff_df.sort_values(by=sort_column, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "\n",
    "# alpha값에 따른 회귀 모델의 폴드 평균 RMSE를 출력하고 회귀 계수값들을 DataFrame으로 반환 \n",
    "def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None, \n",
    "                        verbose=True, return_coeff=True):\n",
    "    coeff_df = pd.DataFrame()\n",
    "    if verbose : print('####### ', model_name , '#######')\n",
    "    for param in params:\n",
    "        if model_name =='Ridge': model = Ridge(alpha=param)\n",
    "        elif model_name =='Lasso': model = Lasso(alpha=param)\n",
    "        elif model_name =='ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7)\n",
    "        neg_mse_scores = cross_val_score(model, X_data_n, \n",
    "                                             y_target_n, scoring=\"neg_mean_squared_error\", cv = 5)\n",
    "        avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))\n",
    "        print('alpha {0}일 때 5 폴드 세트의 평균 RMSE: {1:.3f} '.format(param, avg_rmse))\n",
    "        # cross_val_score는 evaluation metric만 반환하므로 모델을 다시 학습하여 회귀 계수 추출\n",
    "        \n",
    "        model.fit(X_data_n , y_target_n)\n",
    "        if return_coeff:\n",
    "            # alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame의 컬럼으로 추가. \n",
    "            coeff = pd.Series(data=model.coef_ , index=X_data_n.columns )\n",
    "            colname='alpha:'+str(param)\n",
    "            coeff_df[colname] = coeff\n",
    "    \n",
    "    return coeff_df\n",
    "# end of get_linear_regre_eval\n",
    "# 라쏘에 사용될 alpha 파라미터의 값들을 정의하고 get_linear_reg_eval() 함수 호출\n",
    "lasso_alphas = [ 0.07, 0.1, 0.5, 1, 3]\n",
    "coeff_lasso_df =get_linear_reg_eval('Lasso', params=lasso_alphas, X_data_n=X_data, y_target_n=y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반환된 coeff_lasso_df를 첫번째 컬럼순으로 내림차순 정렬하여 회귀계수 DataFrame출력\n",
    "sort_column = 'alpha:'+str(lasso_alphas[0])\n",
    "coeff_lasso_df.sort_values(by=sort_column, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엘라스틱넷에 사용될 alpha 파라미터의 값들을 정의하고 get_linear_reg_eval() 함수 호출\n",
    "# l1_ratio는 0.7로 고정\n",
    "elastic_alphas = [ 0.07, 0.1, 0.5, 1, 3]\n",
    "coeff_elastic_df =get_linear_reg_eval('ElasticNet', params=elastic_alphas,\n",
    "                                      X_data_n=X_data, y_target_n=y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반환된 coeff_elastic_df를 첫번째 컬럼순으로 내림차순 정렬하여 회귀계수 DataFrame출력\n",
    "sort_column = 'alpha:'+str(elastic_alphas[0])\n",
    "coeff_elastic_df.sort_values(by=sort_column, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "\n",
    "# method는 표준 정규 분포 변환(Standard), 최대값/최소값 정규화(MinMax), 로그변환(Log) 결정\n",
    "# p_degree는 다향식 특성을 추가할 때 적용. p_degree는 2이상 부여하지 않음. \n",
    "def get_scaled_data(method='None', p_degree=None, input_data=None):\n",
    "    if method == 'Standard':\n",
    "        scaled_data = StandardScaler().fit_transform(input_data)\n",
    "    elif method == 'MinMax':\n",
    "        scaled_data = MinMaxScaler().fit_transform(input_data)\n",
    "    elif method == 'Log':\n",
    "        scaled_data = np.log1p(input_data)\n",
    "    else:\n",
    "        scaled_data = input_data\n",
    "\n",
    "    if p_degree != None:\n",
    "        scaled_data = PolynomialFeatures(degree=p_degree, \n",
    "                                         include_bias=False).fit_transform(scaled_data)\n",
    "    \n",
    "    return scaled_data\n",
    "# Ridge의 alpha값을 다르게 적용하고 다양한 데이터 변환방법에 따른 RMSE 추출. \n",
    "alphas = [0.1, 1, 10, 100]\n",
    "#변환 방법은 모두 6개, 원본 그대로, 표준정규분포, 표준정규분포+다항식 특성\n",
    "# 최대/최소 정규화, 최대/최소 정규화+다항식 특성, 로그변환 \n",
    "scale_methods=[(None, None), ('Standard', None), ('Standard', 2), \n",
    "               ('MinMax', None), ('MinMax', 2), ('Log', None)]\n",
    "for scale_method in scale_methods:\n",
    "    X_data_scaled = get_scaled_data(method=scale_method[0], p_degree=scale_method[1], \n",
    "                                    input_data=X_data)\n",
    "    print(X_data_scaled.shape, X_data.shape)\n",
    "    print('\\n## 변환 유형:{0}, Polynomial Degree:{1}'.format(scale_method[0], scale_method[1]))\n",
    "    get_linear_reg_eval('Ridge', params=alphas, X_data_n=X_data_scaled, \n",
    "                        y_target_n=y_target, verbose=False, return_coeff=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# StandardScaler( )로 평균이 0, 분산 1로 데이터 분포도 변환\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(cancer.data)\n",
    "\n",
    "X_train , X_test, y_train , y_test = train_test_split(data_scaled, cancer.target, test_size=0.3, random_state=0)\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# 로지스틱 회귀를 이용하여 학습 및 예측 수행.\n",
    "# solver 인자값을 생성자로 입력하지 않으면 solver='lbfgs'\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_preds = lr_clf.predict(X_test)\n",
    "\n",
    "# accuracy와 roc_auc 측정\n",
    "print('accuracy: {0:.3f}, roc_auc:{1:.3f}'.format(accuracy_score(y_test, lr_preds),\n",
    "                                                  roc_auc_score(y_test , lr_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olvers = ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga']\n",
    "\n",
    "# 여러개의 solver 값별로 LogisticRegression 학습 후 성능 평가\n",
    "for solver in solvers:\n",
    "    lr_clf = LogisticRegression(solver=solver, max_iter=600)\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    lr_preds = lr_clf.predict(X_test)\n",
    "    \n",
    "    # accuracy와 roc_auc 측정\n",
    "    print('solver:{0}, accuracy: {1:.3f}, roc_auc:{2:.3f}'.format(solver,\n",
    "                                                                  accuracy_score(y_test, lr_preds),\n",
    "                                                                  roc_auc_score(y_test , lr_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params={'solver':['liblinear', 'lbfgs'],\n",
    "        'penalty':['l2', 'l1'],\n",
    "        'C':[0.01, 0.1, 1, 1, 5, 10]}\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "\n",
    "grid_clf = GridSearchCV(lr_clf, param_grid=params, scoring='accuracy', cv=3 )\n",
    "grid_clf.fit(data_scaled, cancer.target)\n",
    "print('최적 하이퍼 파라미터:{0}, 최적 평균 정확도:{1:.3f}'.format(grid_clf.best_params_,\n",
    "                                                  grid_clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 보스턴 데이터 세트 로드\n",
    "bostonDF= pd.read_csv('./dataset/BostonHousing.csv')\n",
    "\n",
    "# boston dataset의 target array는 주택 가격임. medv를 PRICE 컬럼으로 DataFrame에 추가함\n",
    "bostonDF.rename(columns={'medv':'PRICE'}, inplace=True)\n",
    "\n",
    "y_target = bostonDF['PRICE']\n",
    "X_data = bostonDF.drop(['PRICE'], axis=1,inplace=False)\n",
    "\n",
    "rf = RandomForestRegressor(random_state=0, n_estimators=1000)\n",
    "neg_mse_scores = cross_val_score(rf, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5)\n",
    "rmse_scores  = np.sqrt(-1 * neg_mse_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "\n",
    "print(' 5 교차 검증의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 2))\n",
    "print(' 5 교차 검증의 개별 RMSE scores : ', np.round(rmse_scores, 2))\n",
    "print(' 5 교차 검증의 평균 RMSE : {0:.3f} '.format(avg_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_cv_prediction(model, X_data, y_target):\n",
    "    neg_mse_scores = cross_val_score(model, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5)\n",
    "    rmse_scores  = np.sqrt(-1 * neg_mse_scores)\n",
    "    avg_rmse = np.mean(rmse_scores)\n",
    "    print('##### ',model.__class__.__name__ , ' #####')\n",
    "    print(' 5 교차 검증의 평균 RMSE : {0:.3f} '.format(avg_rmse))\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "dt_reg = DecisionTreeRegressor(random_state=0, max_depth=4)\n",
    "rf_reg = RandomForestRegressor(random_state=0, n_estimators=1000)\n",
    "gb_reg = GradientBoostingRegressor(random_state=0, n_estimators=1000)\n",
    "xgb_reg = XGBRegressor(n_estimators=1000)\n",
    "lgb_reg = LGBMRegressor(n_estimators=1000)\n",
    "\n",
    "# 트리 기반의 회귀 모델을 반복하면서 평가 수행 \n",
    "models = [dt_reg, rf_reg, gb_reg, xgb_reg, lgb_reg]\n",
    "for model in models:  \n",
    "    get_model_cv_prediction(model, X_data, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "rf_reg = RandomForestRegressor(n_estimators=1000)\n",
    "\n",
    "# 앞 예제에서 만들어진 X_data, y_target 데이터 셋을 적용하여 학습합니다.   \n",
    "rf_reg.fit(X_data, y_target)\n",
    "\n",
    "feature_series = pd.Series(data=rf_reg.feature_importances_, index=X_data.columns )\n",
    "feature_series = feature_series.sort_values(ascending=False)\n",
    "sns.barplot(x= feature_series, y=feature_series.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "bostonDF_sample = bostonDF[['rm','PRICE']]\n",
    "bostonDF_sample = bostonDF_sample.sample(n=100,random_state=0)\n",
    "print(bostonDF_sample.shape)\n",
    "plt.figure()\n",
    "plt.scatter(bostonDF_sample.rm , bostonDF_sample.PRICE,c=\"darkorange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 선형 회귀와 결정 트리 기반의 Regressor 생성. DecisionTreeRegressor의 max_depth는 각각 2, 7\n",
    "lr_reg = LinearRegression()\n",
    "rf_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "rf_reg7 = DecisionTreeRegressor(max_depth=7)\n",
    "\n",
    "# 실제 예측을 적용할 테스트용 데이터 셋을 4.5 ~ 8.5 까지 100개 데이터 셋 생성. \n",
    "X_test = np.arange(4.5, 8.5, 0.04).reshape(-1, 1)\n",
    "\n",
    "# 보스턴 주택가격 데이터에서 시각화를 위해 피처는 RM만, 그리고 결정 데이터인 PRICE 추출\n",
    "X_feature = bostonDF_sample['rm'].values.reshape(-1,1)\n",
    "y_target = bostonDF_sample['PRICE'].values.reshape(-1,1)\n",
    "\n",
    "# 학습과 예측 수행. \n",
    "lr_reg.fit(X_feature, y_target)\n",
    "rf_reg2.fit(X_feature, y_target)\n",
    "rf_reg7.fit(X_feature, y_target)\n",
    "\n",
    "pred_lr = lr_reg.predict(X_test)\n",
    "pred_rf2 = rf_reg2.predict(X_test)\n",
    "pred_rf7 = rf_reg7.predict(X_test)\n",
    "fig , (ax1, ax2, ax3) = plt.subplots(figsize=(14,4), ncols=3)\n",
    "\n",
    "# X축값을 4.5 ~ 8.5로 변환하며 입력했을 때, 선형 회귀와 결정 트리 회귀 예측 선 시각화\n",
    "# 선형 회귀로 학습된 모델 회귀 예측선 \n",
    "ax1.set_title('Linear Regression')\n",
    "ax1.scatter(bostonDF_sample.rm, bostonDF_sample.PRICE, c=\"darkorange\")\n",
    "ax1.plot(X_test, pred_lr,label=\"linear\", linewidth=2 )\n",
    "\n",
    "# DecisionTreeRegressor의 max_depth를 2로 했을 때 회귀 예측선 \n",
    "ax2.set_title('Decision Tree Regression: \\n max_depth=2')\n",
    "ax2.scatter(bostonDF_sample.rm, bostonDF_sample.PRICE, c=\"darkorange\")\n",
    "ax2.plot(X_test, pred_rf2, label=\"max_depth:3\", linewidth=2 )\n",
    "\n",
    "# DecisionTreeRegressor의 max_depth를 7로 했을 때 회귀 예측선 \n",
    "ax3.set_title('Decision Tree Regression: \\n max_depth=7')\n",
    "ax3.scatter(bostonDF_sample.rm, bostonDF_sample.PRICE, c=\"darkorange\")\n",
    "ax3.plot(X_test, pred_rf7, label=\"max_depth:7\", linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "bike_df = pd.read_csv('./dataset/bike_train.csv')\n",
    "print(bike_df.shape)\n",
    "bike_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열을 datetime 타입으로 변경. \n",
    "bike_df['datetime'] = bike_df.datetime.apply(pd.to_datetime)\n",
    "\n",
    "# datetime 타입에서 년, 월, 일, 시간 추출\n",
    "bike_df['year'] = bike_df.datetime.apply(lambda x : x.year)\n",
    "bike_df['month'] = bike_df.datetime.apply(lambda x : x.month)\n",
    "bike_df['day'] = bike_df.datetime.apply(lambda x : x.day)\n",
    "bike_df['hour'] = bike_df.datetime.apply(lambda x: x.hour)\n",
    "bike_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['datetime','casual','registered']\n",
    "bike_df.drop(drop_columns, axis=1,inplace=True)\n",
    "fig, axs = plt.subplots(figsize=(16, 8), ncols=4, nrows=2)\n",
    "cat_features = ['year', 'month','season','weather','day', 'hour', 'holiday','workingday']\n",
    "# cat_features에 있는 모든 칼럼별로 개별 칼럼값에 따른 count의 합을 barplot으로 시각화\n",
    "for i, feature in enumerate(cat_features):\n",
    "    row = int(i/4)\n",
    "    col = i%4\n",
    "    # 시본의 barplot을 이용해 칼럼값에 따른 count의 합을 표현\n",
    "    sns.barplot(x=feature, y='count', data=bike_df, ax=axs[row][col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# log 값 변환 시 NaN등의 이슈로 log() 가 아닌 log1p() 를 이용하여 RMSLE 계산\n",
    "def rmsle(y, pred):\n",
    "    log_y = np.log1p(y)\n",
    "    log_pred = np.log1p(pred)\n",
    "    squared_error = (log_y - log_pred) ** 2\n",
    "    rmsle = np.sqrt(np.mean(squared_error))\n",
    "    return rmsle\n",
    "\n",
    "# 사이킷런의 mean_square_error() 를 이용하여 RMSE 계산\n",
    "def rmse(y,pred):\n",
    "    return np.sqrt(mean_squared_error(y,pred))\n",
    "\n",
    "# MSE, RMSE, RMSLE 를 모두 계산 \n",
    "def evaluate_regr(y,pred):\n",
    "    rmsle_val = rmsle(y,pred)\n",
    "    rmse_val = rmse(y,pred)\n",
    "    # MAE 는 scikit learn의 mean_absolute_error() 로 계산\n",
    "    mae_val = mean_absolute_error(y,pred)\n",
    "    print('RMSLE: {0:.3f}, RMSE: {1:.3F}, MAE: {2:.3F}'.format(rmsle_val, rmse_val, mae_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split , GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression , Ridge , Lasso\n",
    "\n",
    "y_target = bike_df['count']\n",
    "X_features = bike_df.drop(['count'],axis=1,inplace=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0)\n",
    "\n",
    "lr_reg = LinearRegression()\n",
    "lr_reg.fit(X_train, y_train)\n",
    "pred = lr_reg.predict(X_test)\n",
    "\n",
    "evaluate_regr(y_test ,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_error_data(y_test, pred, n_tops = 5):\n",
    "    # DataFrame에 컬럼들로 실제 대여횟수(count)와 예측 값을 서로 비교 할 수 있도록 생성. \n",
    "    result_df = pd.DataFrame(y_test.values, columns=['real_count'])\n",
    "    result_df['predicted_count']= np.round(pred)\n",
    "    result_df['diff'] = np.abs(result_df['real_count'] - result_df['predicted_count'])\n",
    "    # 예측값과 실제값이 가장 큰 데이터 순으로 출력. \n",
    "    print(result_df.sort_values('diff', ascending=False)[:n_tops])\n",
    "    \n",
    "get_top_error_data(y_test,pred,n_tops=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target.hist()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
